{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00bf857",
   "metadata": {},
   "source": [
    "# Attention\n",
    "## Why Attention\n",
    "Attention solves the problem of long range information transfer. Although LSTM and GRU can help with this problem, but only one connection between Encoder and Decoder presents the bottleneck problem, which means the last computation of the Encoder must contain all information of the sentence so that Encoder can process properly. Instead, attention allows Decoder to directly contact the activations of every word.\n",
    "\n",
    "Ps. Attention changes the way output is generated, meaning LSTM or GRU can be used as Encoder align with Attention model.\n",
    "\n",
    "## Computational Process\n",
    "For each word generated by Encoder, it uses a weighted mean across all Encoder activations and \"attends to\" the most relevant activations. The parameter here is $\\alpha^{<t, t'>}$, where $t$ is the index for outputs, and $t'$ is the index for activations.\\\n",
    "__$\\alpha^{<t,t'>}$ is the amound of attention $y^{<t>}$ should pay to a^{<t'>}__ \\\n",
    "The formula will be\n",
    "$$c^{<1>} = \\alpha^{<1,1>}a^{<1>} + \\alpha^{<1,2>}a^{<2>} +\\dots+ \\alpha^{<1,t'>}a^{<t'>}$$\n",
    "$$S^{<0>} + c^{<1>} \\to S^{<1>} \\to word^{<1>}$$\n",
    "Similarly\n",
    "$$c^{<t>} = \\sum_{t'}\\alpha^{<t,t'>}a^{<t'>}$$\n",
    "$$S^{<t-1>} + c^{<t>} \\to S^{<t>} \\to word^{<t>}$$\n",
    "The process continues until the output predicts \\<EOS>\n",
    "\n",
    "Notably, the sum of $\\alpha^{<1,t'>}$ should be equal to one.\n",
    "$$\\sum_{t'}\\alpha^{<t,t'>}=1$$\n",
    "\n",
    "## How to Compute $\\alpha^{<t,t'>}$\n",
    "To ensure the sum of $\\alpha^{<t,t'>}$ over $t'$ equals 1, we need a softmax function\n",
    "$$\\alpha^{<t,t'>} = \\frac{exp(e^{<t,t'>})}{\\sum_{t'=1}^{T_x}exp(e^{<t,t'>})}$$\n",
    "where, $e^{<t,t'>}$ represents how relevant word $y^t$ is to $a^{<t,t'>}$. It is computed through a small neural network with $a^{<t,t'>}$ and $S^{<t-1>}$ as input, so reasonably $e^{<t,t'>}$ contains information from both the activation and the last output.\n",
    "$$\n",
    "S^{<t-1>}\\\\\n",
    "\\downarrow\\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\\to e^{<t,t'>}\\\\\n",
    "\\uparrow\\\\\n",
    "a^{<t,t'>}\n",
    "$$\n",
    "\n",
    "But __why__ does a this neural network work anyway? __It's mythological, just trust the neural network.__\n",
    "\n",
    "## Papers in History\n",
    "The first paper invented Attention and used in CNN.\\\n",
    "[Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate]\\\n",
    "The first paper applied Attention to NLP.\\\n",
    "[Xu et. al., 2015. Show, attend and tell: Neural image caption generation with visual attention]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6571b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
