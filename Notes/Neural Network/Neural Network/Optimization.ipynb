{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280467ea",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0054e6",
   "metadata": {},
   "source": [
    "## Mini-batch\n",
    "\n",
    "$X = [x^{(1)}, x^{(2)}, ..., x^{(m)}]$\n",
    "\n",
    "$Y = [y^{(1)}, y^{(2)}, ..., y^{(m)}]$\n",
    "\n",
    "To accelerate the training when the dataset is too large. Eg. $m$ = 5,000,000\n",
    "\n",
    "Take batch size = 1000\n",
    "\n",
    "In Mini-batch, \n",
    "\n",
    "$X = [X^{\\{1\\}}, X^{\\{2\\}}, ..., X^{\\{5,000\\}}]$, $X^{\\{t\\}}: (n_{x}, 1000)$\n",
    "\n",
    "$Y = [X^{\\{1\\}}, Y^{\\{2\\}}, ..., Y^{\\{5,000\\}}]$, $Y^{\\{t\\}}: (1, 1000)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e0360",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "For t in range(1, 1000):\n",
    "\n",
    "Replace $X$, $Y$ with $X^{\\{t\\}}$ and $Y^{\\{t\\}}$\n",
    "\n",
    "One epoch means one time going through the dataset, which means 5000 gradient descents are taken in one epoch, 5000 times faster than using the entire dataset at once\n",
    "\n",
    "The cost function of mini-batch has noises because each mini-batch can be easier or harder to calculate\n",
    "\n",
    "Mini-batch size should be adequate. If size = m, then learning is too slow; if size = 1 (Stochastic GD), then the gradient runs wild"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b177a2",
   "metadata": {},
   "source": [
    "## Exponentially Weighted (Moving) Averages\n",
    "\n",
    "A method to calculate the average value of a time sequence.\n",
    "\n",
    "$V_{t} = \\beta \\theta_{t} + V_{t-1}\\ V$ is the average value, and $\\theta$ is the current value\n",
    "\n",
    "$\\beta$ is the factor that represents the weight of the current value\n",
    "\n",
    "$\\frac{1}{1-\\beta}\\approx$ the number of previous days considered in the average\n",
    "\n",
    "The reason is that $(1-\\epsilon)^{\\frac{1}{\\epsilon}} = \\frac{1}{e}$, so the weight of the value before $\\frac{1}{1-\\beta}\\approx$ days drop to approximately $\\frac{1}{3}$\n",
    "\n",
    "Eg. $\\beta = 0.9,\\ (1-(1-0.9))^{\\frac{1}{1 - 0.9}}\\ \\approx 0.35\\ \\approx \\frac{1}{e}$, so days $=$ 10\n",
    "\n",
    "In terms of coding:\\\n",
    "$\n",
    "V := 0\\\\\n",
    "V := \\beta V + (1-\\beta)\\theta_t$\n",
    "\n",
    "### Bias Correction\n",
    "Bias comes from $V_0 = 0$, so initial values are much less than actual values\n",
    "\n",
    "Correcting the bias by adding the term $V_t := \\frac{V_t}{1-\\beta^t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44ee16",
   "metadata": {},
   "source": [
    "## Gradient Descent with Momentum\n",
    "Taking previous gradients into consideration, rather than calculating each gradient independently\n",
    "\n",
    "$V_{dW} := 0$, $V_{db} := 0$\n",
    "\n",
    "$V_{dW} := \\beta V_{dW} + (1-\\beta)dW$\n",
    "\n",
    "$V_{db} := \\beta V_{db} + (1-\\beta)db$\n",
    "\n",
    "Instead of $W := W - \\alpha dW$ and $b := b - \\alpha db$\n",
    "\n",
    "$W := W - \\alpha V_{dW}$ and $b := b - \\alpha V_{db}$\n",
    "\n",
    "Explanations:\n",
    "1. To reduce the deviation in the oscillation direction and add up the gradient in the accumulation direction\n",
    "2. A ball rolling down a bowl: $V_{dW}$ and $V_{db}$ are velocities, $dW$ and $db$ are accelerations, and $\\beta$ is friction\n",
    "\n",
    "Note:\n",
    "1. Usually don't use the bias correction term\n",
    "2. $\\beta = 0.9$ is a common good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769ffe4",
   "metadata": {},
   "source": [
    "## RMSprop (Root Mean Square)\n",
    "$S_{dW} := 0$, $S_{db} := 0$\n",
    "\n",
    "$S_{dW} := \\beta_2 S_{dW} + (1-\\beta_2)dW^2$\n",
    "\n",
    "$S_{db} := \\beta_2 S_{db} + (1-\\beta_2)db^2$\n",
    "\n",
    "Instead of $W := W - \\alpha dW$ and $b := b - \\alpha db$\n",
    "\n",
    "$W := W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}}$ and $b := b - \\alpha \\frac{db}{\\sqrt{S_{db}}}$\n",
    "\n",
    "If $W$ is the length and $b$ is the width of an ellipse, the RMS helps to reduce $db$ and increase $dW$\n",
    "\n",
    "To prevent gradient explosion, \n",
    "\n",
    "$W := W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}\\ \\ +\\ \\epsilon}$ and $b := b - \\alpha \\frac{db}{\\sqrt{S_{db}}\\ \\ +\\ \\epsilon}$\n",
    "\n",
    "where $\\epsilon = 10^{-8}$\n",
    "\n",
    "After optimization, a greater learning rate $\\alpha$ can be applied to training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919ad23",
   "metadata": {},
   "source": [
    "## Adam Optimization Algorithm (Momentum + RMS)\n",
    "$V_{dW} := 0$, $V_{db} := 0$, $S_{dW} := 0$, $S_{db} := 0$\n",
    "\n",
    "$V_{dW} := \\beta_1 V_{dW} + (1-\\beta_1)dW$, $V_{db} := \\beta_1 V_{db} + (1-\\beta_1)db$\n",
    "\n",
    "$S_{dW} := \\beta_2 S_{dW} + (1-\\beta_2)dW^2$, $S_{db} := \\beta_2 S_{db} + (1-\\beta_2)db^2$\n",
    "\n",
    "$V^{correct}_{dW} = \\frac{V_{dW}}{1-\\beta_1^t}$, $V^{correct}_{db} = \\frac{V_{db}}{1-\\beta_1^t}$\n",
    "\n",
    "$S^{correct}_{dW} = \\frac{S_{dW}}{1-\\beta_2^t}$, $S^{correct}_{db} = \\frac{S_{db}}{1-\\beta_2^t}$\n",
    "\n",
    "$W = W - \\alpha \\frac{V^{correct}_{dW}}{\\sqrt{S^{correct}_{dW}}\\ \\ \\ \\ +\\ \\epsilon}$\n",
    "\n",
    "Usually $\\beta_1 = 0.9,\\ \\beta_2 = 0.999,\\ \\epsilon = 10^{-8}$(Almost useless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e35df1",
   "metadata": {},
   "source": [
    "## Learning Rate Decay\n",
    "Slowly reduce $\\alpha$ as training goes on can help the algorithm to converge to the minimum point\n",
    "\n",
    "Ways of Decay\n",
    "1. $\\alpha = \\frac{1}{1\\ +\\ decay\\_rate\\ *\\ epoch\\_num}\\alpha_0$\n",
    "2. $\\alpha = 0.95\\ ^{epoch\\_num}\\alpha_0\\ \\ $  Exponentially Decay\n",
    "3. $\\alpha = \\frac{k}{\\sqrt{epoch\\_num}}\\alpha_0$ or $\\alpha = \\frac{k}{\\sqrt{t}}\\alpha_0$\n",
    "4. Discrete Staircase, a different learning rate for each step\n",
    "5. Manually control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d4b67",
   "metadata": {},
   "source": [
    "## Saddle Points\n",
    "When gradient equals zero, some parameters are concave, some are convex, while local optima are rare\n",
    "\n",
    "The problem is the slow descending rate near plateau, while stucking in local optima is unlikely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1fa16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
