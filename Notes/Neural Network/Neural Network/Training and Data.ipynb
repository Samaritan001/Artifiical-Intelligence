{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f3715",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3b1a5",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "Add $\\frac{\\lambda}{2m}\\lVert W \\rVert^{[2]}_{[F]}$ term to Cost Function $J(W, b)$\n",
    "\n",
    "This adds $\\frac{\\lambda}{m}W^{[l]}$ to $dW^[l]$\n",
    "\n",
    "Thus $W^{[l]} := W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha dW^{[l]}_{original} = (1 - \\alpha \\frac{\\lambda}{m})W^{[l]} - \\alpha dW^{[l]}_{original}$\n",
    "\n",
    "Where $\\lambda$ is the regularization factor, a hyperparameter\n",
    "\n",
    "This regularization tends to generally reduce $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8154032",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Randomly shut off units base on keep-prob (probability to not dropout for a layer of units)\n",
    "\n",
    "$dn = np.random.rand(a^{[l]}.shape([0]), a^{[l]}.shape([1])) < keep-prob$\n",
    "\n",
    "$a^{[l]} *= dn$\n",
    "\n",
    "$a^{[l]} /= keep-prob$ to maintain the expected value of $a^{[l]}$\n",
    "\n",
    "keep-prob is a hyperparameter, can be different in different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ebb053",
   "metadata": {},
   "source": [
    "## Other Methods\n",
    "Early Stopping, combines lowering cost and variance together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894d115",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "$x := \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "Standardized data make gradient descent faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9adcd",
   "metadata": {},
   "source": [
    "# Gradient Explosion and Gradient Vanishing\n",
    "\n",
    "When there are too many hidden layers, the predicted value can end up being very large or small, so as the gradient.\n",
    "\n",
    "To partially solve this, $W$ can be weight initialized.\n",
    "\n",
    "$W^{[l]} = np.random.rand(shape) * np.sqrt(\\frac{\\lambda}{n^{[l-1]}})$\n",
    "\n",
    "$np.sqrt(\\frac{\\lambda}{n^{[l-1]}})$ is the standard deviation, and $n^{[l-1]}$ is the degree of $w$ in each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28fa1c",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "To check if the actual gradient descent is the same as predicted.\n",
    "\n",
    "$J(W, b) = J(\\theta)$ put all features $W^{[l]}, b^{[l]}$ into one vector $\\theta$\n",
    "\n",
    "$d\\theta^{[i]}_{approx} = \\frac{J(\\theta_{1}, \\theta_{2}, ..., \\theta_{i}+\\epsilon, ...) - J(\\theta_{1}, \\theta_{2}, ..., \\theta_{i}-\\epsilon, ...)}{2\\epsilon} \\approx d\\theta^{[i]} = \\frac{\\partial{J}}{\\partial{\\theta_{i}}}$\n",
    "\n",
    "Check the difference between vector $d\\theta_{approx}$ and $d\\theta$\n",
    "\n",
    "$\\frac{\\lVert d\\theta_{approx}\\ \\ \\ \\  - d\\theta \\rVert_{2}}{\\lVert d\\theta_{approx}\\ \\ \\ \\ \\rVert_{2}\\ +\\ \\lVert d\\theta \\rVert_{2}}$\n",
    "\n",
    "The difference should be less than $10^{-7}$, otherwise check the model. The function can separately check $W$ and $b$ to determine the location of the error.\n",
    "\n",
    "Doesn't apply to Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9d4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
