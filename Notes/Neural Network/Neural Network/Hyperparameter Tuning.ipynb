{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf3a474",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "## Hyperparameters - list by importance\n",
    "1. $\\alpha$, learning rate\n",
    "2. $\\beta$, momentum term; #hidden units; #mini-batch size\n",
    "3. #layers; learning-rate decay\n",
    "4. $\\beta_1$, momentum term; $\\beta_2$, RMS term; $\\epsilon$, correction term\n",
    "\n",
    "## Hyperparameter search\n",
    "Choose random points instead of using a grid.\n",
    "\n",
    "Continue to randomly choose hyperparameter values from a smaller optimal area\n",
    "\n",
    "### Appropriate Scale\n",
    "In the scale [0.0001, 1], choose equivalently between each digit.\n",
    "\n",
    "$r = -4*np.random.rand()$, $r\\ \\epsilon\\ [-4, 0]$\n",
    "\n",
    "$\\alpha = 10^r$, $\\alpha\\ \\epsilon\\ [10^{-4}, 1]$\n",
    "\n",
    "the boundaries of $r [a, b]$ are logrithmic\n",
    "\n",
    "As for $\\beta$, $0.9 \\sim 0.999$ for example, the scale is regard to $1-\\beta$\n",
    "\n",
    "In this case, $r\\ \\epsilon\\ [-3, -1]$, $\\beta = 1 - 10^r$, $\\beta\\ \\epsilon\\ [0.9, 0.999]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038533aa",
   "metadata": {},
   "source": [
    "## Two approaches to tune a model\n",
    "1. Panda: babysit a model by gradually tuning each hyperparameter - for low computational resources\n",
    "2. Caviar: parallelly trying different sets of hyperparameters and choose the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1214910",
   "metadata": {},
   "source": [
    "## Batch Normalization - Normalizing Activations in a Network\n",
    "To stabilize later layers' training, maintain the standard deviation and mean even though input might change\n",
    "\n",
    "As normalizing $X$, normalize $Z^{[l]}$ to improve the learning of $W^{[l+1]}$ and $b^{[l+1]}$\n",
    "\n",
    "Take $Z^{[l]} = [z^{(1)}, z^{(2)}, ..., z^{(m)}]$\n",
    "\n",
    "$\\mu = \\frac{1}{m}\\sum_{i}{}z^{(i)}$, the sum is regard to $m$ examples, $\\mu$ is a vector of $(n^{[l]}, 1)$\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{m}\\sum_{i}{}(z^{(i)} - \\mu)^2$, \n",
    "\n",
    "$z^{(i)}_{norm} = \\frac{z^{(i) - \\mu}}{\\sqrt{\\sigma^2\\ +\\ \\epsilon}}$, has mean = 0 and standard deviation = 1\n",
    "\n",
    "$\\widetilde{z}^{(i)} = \\gamma z^{(i)}_{norm} + \\beta$, has mean = $\\beta$ and standard deviation = $\\gamma$; if $\\gamma = \\sqrt{\\sigma^2 + \\epsilon}$, $\\beta = \\mu$, $\\widetilde{z}^{(i)} = z^{(i)}_{norm}$\n",
    "\n",
    "$\\beta^{[l]}$ is for batch normalization, different from optimization hyperparameters $\\beta, \\beta_1, \\beta_2$\n",
    "\n",
    "$\\epsilon$ is used to prevent $\\sigma = 0$\n",
    "\n",
    "$b^{[l]}$ is useless in batch normalization, for adding the same constant $b^{[l]}_i$ to one row of data doesn't change the z-score\n",
    "\n",
    "Has a little regularization effect as \"drop out\", since batch norm adds noise to units\n",
    "\n",
    "### Batch Norm at Test Time\n",
    "Instead of mini batches, test data are one at a time\n",
    "\n",
    "In this case, $\\mu$ and $\\sigma$ should be using values derived from training\n",
    "\n",
    "For layer $l$, $X^{\\{t\\}} \\to \\mu^{\\{t\\}[l]}, \\sigma^{\\{t\\}[l]}$\n",
    "\n",
    "$\\mu =$ exponentially weighted average of $\\mu^{\\{t\\}[l]}$, and $\\sigma =$ exponentially weighted average of $\\sigma^{\\{t\\}[l]}$, then use in calculating $\\widetilde{z}^{(i)}$\n",
    "\n",
    "(Is $\\mu$ with respect to one layer or it's a constant across layers?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba105a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd240825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
