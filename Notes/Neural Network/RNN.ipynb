{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a569c29",
   "metadata": {},
   "source": [
    "# Sequence Model\n",
    "One of input and output is a sequence model.\n",
    "\n",
    "## Notations\n",
    "Input: $x: x^{<1>}, x^{<2>}, ..., x^{<t>}, ..., x^{<n>}$\\\n",
    "Output: $y: y^{<1>}, y^{<2>}, ..., y^{<t>}, ..., y^{<n>}$\\\n",
    "Number of features (sequence length) in $x: T_x$\\\n",
    "Number of features in $y: T_y$\\\n",
    "Feature $t$ in $i$th training example: $x^{(i)<t>}$\\\n",
    "Feature $t$ in $i$th output: $y^{(i)<t>}$\\\n",
    "Sequence length of $i$th training example: $T_x^{(i)}$\\\n",
    "Sequence length of $i$th output: $T_x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495d6af",
   "metadata": {},
   "source": [
    "# Word Representation\n",
    "## One-hot Method\n",
    "Build a dictionary with, say 10,000, words.\\\n",
    "Construct a vector for each word from the input, the vector elements are zeros except for the word, which is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02c322",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "## Why not traditional neural network\n",
    "__Problems__\n",
    "1. Length of input and output is variable.\n",
    "2. Cannot flexibly transfer information learned from one part of text to another part.\n",
    "3. Too many parameters to train.\n",
    "\n",
    "## Structure\n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{y}^{<1>}$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{y}^{<2>}$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{y}^{<3>}$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{y}^{<T_y>}$\\\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\\\n",
    "$a^{<0>}\\ \\to\\ $\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\ \\to\\ a^{<1>}\\ \\to\\ $\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\ \\to\\ a^{<2>}\\ \\to\\ $\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\ \\to\\ a^{<3>}\\ \\to\\ ...\\ \\to\\ $\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\n",
    "$\\\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\\\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{x}^{<1>}$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{x}^{<2>}$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{x}^{<3>}$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{x}^{<T_x>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e60d40",
   "metadata": {},
   "source": [
    "The network gives a prediction after taking in one word, and the prediction of the prior words will affect the laters.\\\n",
    "The parameters are $W_{ax}$ for inputs, $W_{aa}$ for activations, and $W_{ya}$ for predictions. They are unchanged for each word, which is why the network is ___Recurrent___.\n",
    "\n",
    "Downside: only words before the target work is considered. Solution: Bidirectional RNN (BRNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43053b0b",
   "metadata": {},
   "source": [
    "## Foward Propagation\n",
    "__Note:__ parameter $W_{ij}$ means $W$ times $j$ to calculate $i$.\n",
    "\n",
    "$a^{<0>} = 0$\\\n",
    "$a^{<1>} = g(W_{aa}a^{<0>} + W_{ax}x^{<1>} + b_a)$, $g$ is often $tanh$\\\n",
    "$\\hat{y}^{<1>} = g_2(W_{ya}a^{<1>} + b_y)$, $g_2$ depends on the type of prediction, either softmax, sigmoid, or others\n",
    "\n",
    "Generally\\\n",
    "$a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$\\\n",
    "$\\hat{y}^{<t>} = g_2(W_{ya}a^{<t>} + b_y)$\n",
    "\n",
    "Simplified\\\n",
    "$a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a)$\\\n",
    "$\\hat{y}^{<t>} = g_2(W_ya^{<t>} + b_y)$\\\n",
    "Where $W_a = [W_{aa}|W_{ax}]$, $W_{aa}: (100,100), W_{ax}: (100,10000), W_a: (100,10100)$\\\n",
    "$[a^{<t-1>}, x^{<t>}] = $\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    a^{<t-1>} \\\\\n",
    "    x^{<t>}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf223f",
   "metadata": {},
   "source": [
    "## Back Propagation Through Time\n",
    "Through time: as Foward Propagation is from left to right, Back Propagation is from right to left, meanwhile t decreases.\n",
    "\n",
    "Each prediction has a loss $L^{<t>} = -y^{<t>}log\\hat{y}^{<t>}-(1-y^{<t>})log(1-\\hat{y}^{<t>})$, overall loss is $L(\\hat{y}, y) = \\sum_{t=1}^{T_y}{L^{<t>}(\\hat{y}^{<t>}, y^{<t>})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2564b39",
   "metadata": {},
   "source": [
    "# Different Types of RNN Architectures\n",
    "Many-to-one (only outputs $\\hat{y}$ in the last round)\\\n",
    "One-to-many (only inputs in the first round, feed the last output $\\hat{y}^{<t-1>}$ as $x^{<t>}$)\n",
    "\n",
    "## Many-to-many ($T_x \\neq T_y$)\n",
    "Encoder of length $T_x$ plus Decoder of length $T_y$\n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{y}^{<1>}$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{y}^{<T_y>}$\\\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\\\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\ \\to\\ \\dots\\ \\to\\ $\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\ \\to\\ $\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\\ \\to\\ \\dots\\ \\to\\ $\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box \\\\\n",
    "    \\Box\n",
    "\\end{bmatrix}\n",
    "$\\\n",
    "$\\ \\ \\uparrow$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\uparrow$\\\n",
    "$\\ \\hat{x}^{<1>}$\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{x}^{<T_x>}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35ae05",
   "metadata": {},
   "source": [
    "# Language Modelling\n",
    "## Tokenization\n",
    "Create one-hot vectors for words.\\\n",
    "Add \\<EOS> to the end of sentence as an additional token.\\\n",
    "Mark words outside the dictionary as \\<UNK>.\n",
    "\n",
    "## Training\n",
    "Give all words before word \\<t> as input, and predict the possibility of this word.\\\n",
    "This means $x^{<1>} = \\vec{0}, x^{<t>} = Y^{<t-1>}$ until the last token \\<EOS>\\\n",
    "Total Loss is the softmax loss function over sum up over the sequence outputs.\\\n",
    "The process helps the model to better predict the next word given previous words.\n",
    "\n",
    "## Novel Sequence Sampling\n",
    "Randomly choose a word based on the softmax probabilities. Then input the word to the model, and randomly choose another word based on the output softmax probabilities. Repeat the process until \\<EOS>.\n",
    "\n",
    "Besides word-level modelling, character-based modelling is sometimes used, though expansive and perform worse at info between lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449817ef",
   "metadata": {},
   "source": [
    "# Vanishing Gradients with RNN\n",
    "Traditional RNNs are not good at capturing long-term dependencies of words, such as verb plural form, due to vanishing gradient.\\\n",
    "When vanishing gradients occur, info nearby one word becomes much important than farther info.\n",
    "\n",
    "## Solution: Gated Recurrent Unit (GRU)\n",
    "Intrinsically, GRU maintains a memory cell in a certain timestep, which remain unchanged (influential) through a long term, and forget it at a certain point.\\\n",
    "To maintain the memorial value or replace it with current value depends on function $\\Gamma_u$, which is a value after sigmoid function, thus approximately 0 or 1.\n",
    "\n",
    "__Note:__\n",
    "1. For GRU, memory cell is the same as the hidden state, while they're different in LSTM model.\n",
    "2. The network still gives predicted values as usual, it's only the value passed through each timestep remembered or forgotten.\n",
    "3. To remember more than one memorial feature, memory cell $c^{<t>}$ is often __a vector__, and each timestep updates (or not) all elements of $c^{<t>}$.\n",
    "\n",
    "__Algorithm__ \\\n",
    "$$c^{<t>} = a^{<t>}$$\n",
    "$$\\tilde{c}^{<t>} = tanh(W_c[c^{<t-1>}, x^{<t>}] + b_c)$$\n",
    "$$\\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u)$$\n",
    "$$c^{<t>} = \\Gamma_u \\odot \\tilde{c}^{<t>} + (1-\\Gamma_u) \\odot c^{<t-1>}$$\n",
    "\n",
    "_The above $\\odot$ means element-wise product._ \\\n",
    "To understand easily, we say $\\Gamma_u$ is regarded as 0 or 1, but in real cases it's just an approximation.\\\n",
    "So $c^{<t>}$ is more like a weighted mean between the memory and the current value, and $\\Gamma_u$ represents how important the current value is.\n",
    "\n",
    "Turns out, another gate $\\Gamma_r$ (r for relevance) is used in major GRU models. Why? No reason, it works!\n",
    "$$\\tilde{c}^{<t>} = tanh(\\Gamma_r \\odot W_c[c^{<t-1>}, x^{<t>}] + b_c)$$\n",
    "$$\\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f60394",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "In LSTM, memory cells and hidden states are separately stored.\\\n",
    "LSTM contains 3 controlling gates, update, forget, and output (UFO).\\\n",
    "Update: how much the new memory is added.\\\n",
    "Forget: how much the old memory is preserved.\\\n",
    "Output: how much memory can be seen (transfer to hidden state). ___(What's the point?)___\n",
    "\n",
    "__Algorithm__\n",
    "$$\\tilde{c}^{<t>} = tanh(W_c[c^{<t-1>}, x^{<t>}] + b_c)$$\n",
    "$$\\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u)$$\n",
    "$$\\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f)$$\n",
    "$$\\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o)$$\n",
    "$$c^{<t>} = \\Gamma_u \\odot \\tilde{c}^{<t>} + \\Gamma_f \\odot c^{<t-1>}$$\n",
    "$$a^{<t>} = \\Gamma_o \\odot tanh(c^{<t>})$$\n",
    "\n",
    "Although LSTM is an older model compared to GRU, it usually performs better for having 3 gates. While GRU is sometimes favored for faster computation in large-scale projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c6586",
   "metadata": {},
   "source": [
    "# Bidirectional RNNs\n",
    "Problems occur when previous words are the same.\\\n",
    "Eg.\\\n",
    "He said, \"Teddy bears.\"\\\n",
    "He said, \"Teddy Roosevelt.\"\n",
    "\n",
    "Bidirectional RNN uses two RNNs to combine info from words before and after the target word.\\\n",
    "As the two RNNs meet at word \\<t> and gives prediction,\n",
    "$$\\hat{y}^{<t>} = g(W_y[\\overleftarrow{a}^{<t>}, \\overrightarrow{a}^{<t>}] + b_y)$$\n",
    "Whereas the original RNN is $$\\hat{y}^{<t>} = g(W_y\\overrightarrow{a}^{<t>} + b_y)$$\n",
    "\n",
    "__Problem with BRNN:__ It needs the full text to do bidirectional, so it can't be applied to tasks like real-time speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0274a1",
   "metadata": {},
   "source": [
    "# Deep RNNs\n",
    "Deep RNN are RNNs stacking together, transferring hidden states both horizontally and vertically. Each RNN is a layer.\n",
    "\n",
    "For a hidden state $a^{[l]<t>}$,\n",
    "$$a^{[l]<t>} = g(W_a[a^{[l]<t-1>}, a^{[l-1]<t>}]+b_a^{[l]})$$\n",
    "\n",
    "Deep RNNs are cost expensive, so 3 layers is the ceiling (__but who knows?__).\\\n",
    "However, instead of horizontal stacking, a vertical RNN can replace $y^{<t>}$ for further prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2ffa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
